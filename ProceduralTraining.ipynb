{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ef3325-8417-4e78-9b9a-839cd17b4054",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7661778-fc16-47cc-9c7f-7bc101ff12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55907439-f4a4-4715-9b97-fc11dc6d7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735fe687-c633-4e1e-bfaf-82c6385cedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da154f9-bf53-429f-9d41-c60297ca4fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99beab5-67ac-441f-8a8a-b29323ba9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-lightning==1.9.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f246944-cb2f-48ee-ad8f-d199dbdfba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e292cc4-b724-4de7-a73a-165f9ecfde74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45529f6f-ad36-4516-afba-711fd62da3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        with open('./procedural_generation_training_data/prompt.json', 'rt') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "    \n",
    "        source_filename = item['source']\n",
    "        target_filename = item['target']\n",
    "        prompt = item['prompt']\n",
    "    \n",
    "        source_path = './procedural_generation_training_data/' + source_filename\n",
    "        target_path = './procedural_generation_training_data/' + target_filename\n",
    "    \n",
    "        # Check file existence\n",
    "        if not os.path.exists(source_path):\n",
    "            raise FileNotFoundError(f\"Source file not found: {source_path}\")\n",
    "        if not os.path.exists(target_path):\n",
    "            raise FileNotFoundError(f\"Target file not found: {target_path}\")\n",
    "    \n",
    "        # Load images\n",
    "        source = cv2.imread(source_path)\n",
    "        target = cv2.imread(target_path)\n",
    "    \n",
    "        # Validate image loading\n",
    "        if source is None:\n",
    "            raise ValueError(f\"Failed to load source image: {source_path}\")\n",
    "        if target is None:\n",
    "            raise ValueError(f\"Failed to load target image: {target_path}\")\n",
    "    \n",
    "        # Convert from BGR to RGB\n",
    "        source = cv2.cvtColor(source, cv2.COLOR_BGR2RGB)\n",
    "        target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        # Normalize source images to [0, 1]\n",
    "        source = source.astype(np.float32) / 255.0\n",
    "    \n",
    "        # Normalize target images to [-1, 1]\n",
    "        target = (target.astype(np.float32) / 127.5) - 1.0\n",
    "    \n",
    "        return dict(png=target, txt=prompt, hint=source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2c37f8-2bbc-4da0-8e1c-c92b5193cab3",
   "metadata": {},
   "source": [
    "#### Testing that the dataset was loaded properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9fe6e4-1d22-4f24-9ed0-bee978d733b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset()\n",
    "print(len(dataset))\n",
    "\n",
    "item = dataset[0]\n",
    "png = item['png']\n",
    "txt = item['txt']\n",
    "hint = item['hint']\n",
    "print(txt)\n",
    "print(png.shape)\n",
    "print(hint.shape)\n",
    "\n",
    "# Outputs on the tutorial test machine:\n",
    "# 50000\n",
    "# burly wood circle with orange background\n",
    "# (512, 512, 3)\n",
    "# (512, 512, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ce4e2-9f5e-4d2a-a9eb-bd8f2365a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4acab-1b74-4005-9ad6-dd43836660e0",
   "metadata": {},
   "source": [
    "# Importing the SD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ef6ee-0456-4dcf-9bb8-e823e1e96ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is mostly the same as the tool_add_control_sd2.py file, I wanted to consolidate it all into one notebook\n",
    "# Link to original py file: https://github.com/lllyasviel/ControlNet/blob/main/tool_add_control_sd21.py\n",
    "# Updated script for direct use without command-line arguments\n",
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from share import *\n",
    "from cldm.model import create_model\n",
    "\n",
    "# Hard-coded paths\n",
    "input_path = './models/v2-1_512-ema-pruned.ckpt'\n",
    "output_path = './models/control_sd21_ini.ckpt'\n",
    "\n",
    "# Ensure paths are valid\n",
    "assert os.path.exists(input_path), 'Input model does not exist.'\n",
    "assert not os.path.exists(output_path), 'Output filename already exists.'\n",
    "assert os.path.exists(os.path.dirname(output_path)), 'Output path is not valid.'\n",
    "\n",
    "# Function to extract node names\n",
    "def get_node_name(name, parent_name):\n",
    "    if len(name) <= len(parent_name):\n",
    "        return False, ''\n",
    "    p = name[:len(parent_name)]\n",
    "    if p != parent_name:\n",
    "        return False, ''\n",
    "    return True, name[len(parent_name):]\n",
    "\n",
    "# Load model configuration\n",
    "model = create_model(config_path='./models/cldm_v21.yaml')\n",
    "\n",
    "# Load pretrained weights\n",
    "pretrained_weights = torch.load(input_path)\n",
    "if 'state_dict' in pretrained_weights:\n",
    "    pretrained_weights = pretrained_weights['state_dict']\n",
    "\n",
    "# Initialize the state dictionary for the new model\n",
    "scratch_dict = model.state_dict()\n",
    "target_dict = {}\n",
    "\n",
    "# Map weights from the pretrained model\n",
    "for k in scratch_dict.keys():\n",
    "    is_control, name = get_node_name(k, 'control_')\n",
    "    if is_control:\n",
    "        copy_k = 'model.diffusion_' + name\n",
    "    else:\n",
    "        copy_k = k\n",
    "    if copy_k in pretrained_weights:\n",
    "        target_dict[k] = pretrained_weights[copy_k].clone()\n",
    "    else:\n",
    "        target_dict[k] = scratch_dict[k].clone()\n",
    "        print(f'These weights are newly added: {k}')\n",
    "\n",
    "# Load the updated state dictionary into the model\n",
    "model.load_state_dict(target_dict, strict=True)\n",
    "\n",
    "# Save the updated model\n",
    "torch.save(model.state_dict(), output_path)\n",
    "print('Done.')\n",
    "\n",
    "# should see something like\n",
    "# These weights are newly added ...\n",
    "# These weights are newly added ...\n",
    "# Done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562fbde4-9aa9-4652-86e1-0fafe03c34b0",
   "metadata": {},
   "source": [
    "# Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e3473-6420-4333-99bc-6fac7f21d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from tutorial_train_sd21.py\n",
    "# https://github.com/lllyasviel/ControlNet/blob/main/tutorial_train_sd21.py\n",
    "from share import *\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, Callback\n",
    "from tutorial_dataset import MyDataset\n",
    "from cldm.logger import ImageLogger\n",
    "from cldm.model import create_model, load_state_dict\n",
    "\n",
    "# Configs\n",
    "resume_path = './models/control_sd21_ini.ckpt'  # Pretrained model checkpoint path\n",
    "batch_size = 4\n",
    "logger_freq = 250  # Log progress every 100 steps\n",
    "learning_rate = 1e-5\n",
    "sd_locked = True\n",
    "only_mid_control = False\n",
    "\n",
    "# First, use CPU to load models. Pytorch Lightning will automatically move it to GPUs.\n",
    "model = create_model('./models/cldm_v21.yaml').cpu()\n",
    "model.load_state_dict(load_state_dict(resume_path, location='cpu'))\n",
    "model.learning_rate = learning_rate\n",
    "model.sd_locked = sd_locked\n",
    "model.only_mid_control = only_mid_control\n",
    "\n",
    "# Misc\n",
    "dataset = MyDataset()  # Replace with your dataset implementation\n",
    "dataloader = DataLoader(dataset, num_workers=0, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define a custom logger callback for logging steps\n",
    "class StepLogger(Callback):\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        if trainer.global_step % logger_freq == 0:\n",
    "            print(f\"Step {trainer.global_step}/{trainer.max_steps}\")\n",
    "\n",
    "# Add ModelCheckpoint callback to save the model every 2500 steps\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='./checkpoints',  # Directory to save checkpoints\n",
    "    filename='control_sd21-step={step}-loss={loss:.2f}',  # Filename template\n",
    "    save_top_k=-1,  # Save all checkpoints\n",
    "    every_n_train_steps=2500  # Save every 2500 steps\n",
    ")\n",
    "\n",
    "# Trainer with StepLogger and ModelCheckpoint\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    precision=32,\n",
    "    callbacks=[ImageLogger(batch_frequency=logger_freq), StepLogger(), checkpoint_callback],\n",
    "    max_steps=20000  # Set a maximum number of steps if needed\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.fit(model, dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
